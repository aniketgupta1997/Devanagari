{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Devanagari_training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniketgupta1997/Devanagari/blob/master/Devanagari_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LX6GrfBIGwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "from IPython.display import display, Image\n",
        "from scipy import ndimage\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from six.moves import cPickle as pickle\n",
        "\n",
        "# Config the matlotlib backend as plotting inline in IPython\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Z8iO23I2C5",
        "colab_type": "code",
        "outputId": "852fdd03-336d-40f1-ff7c-0dcabcd5c5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "num_classes = 18\n",
        "np.random.seed(133)\n",
        "\n",
        "def maybe_extract(filename, force=False):\n",
        "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
        "  if os.path.isdir(root) and not force:\n",
        "    # You may override by setting force=True.\n",
        "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
        "  else:\n",
        "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
        "    tar = tarfile.open(filename)\n",
        "    sys.stdout.flush()\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "  data_folders = [\n",
        "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
        "    if os.path.isdir(os.path.join(root, d))]\n",
        "  if len(data_folders) != num_classes:\n",
        "    raise Exception(\n",
        "      'Expected %d folders, one per class. Found %d instead.' % (\n",
        "        num_classes, len(data_folders)))\n",
        "  print(data_folders)\n",
        "  return data_folders\n",
        "  \n",
        "train_folders = maybe_extract('Train.tar')\n",
        "test_folders = maybe_extract('Test.tar')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting data for Train. This may take a while. Please wait.\n",
            "['Train/character_11_taamatar', 'Train/character_13_daa', 'Train/character_16_tabala', 'Train/character_18_da', 'Train/character_1_ka', 'Train/character_21_pa', 'Train/character_25_ma', 'Train/character_26_yaw', 'Train/character_27_ra', 'Train/character_28_la', 'Train/character_29_waw', 'Train/character_30_motosaw', 'Train/character_32_patalosaw', 'Train/character_33_ha', 'Train/character_3_ga', 'Train/character_4_gha', 'Train/character_6_cha', 'Train/character_8_ja']\n",
            "Extracting data for Test. This may take a while. Please wait.\n",
            "['Test/character_11_taamatar', 'Test/character_13_daa', 'Test/character_16_tabala', 'Test/character_18_da', 'Test/character_1_ka', 'Test/character_21_pa', 'Test/character_25_ma', 'Test/character_26_yaw', 'Test/character_27_ra', 'Test/character_28_la', 'Test/character_29_waw', 'Test/character_30_motosaw', 'Test/character_32_patalosaw', 'Test/character_33_ha', 'Test/character_3_ga', 'Test/character_4_gha', 'Test/character_6_cha', 'Test/character_8_ja']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWG2Dcm8Jk4I",
        "colab_type": "code",
        "outputId": "e2b1eeaa-cfa2-4262-8123-d66c5bf584cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "image_size = 32  # Pixel width and height.\n",
        "pixel_depth = 255.0  # Number of levels per pixel.\n",
        "\n",
        "def load_letter(folder, min_num_images):\n",
        "  \"\"\"Load the data for a single letter label.\"\"\"\n",
        "  image_files = os.listdir(folder)\n",
        "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
        "                         dtype=np.float32)\n",
        "  print(folder)\n",
        "  num_images = 0\n",
        "  for image in image_files:\n",
        "    image_file = os.path.join(folder, image)\n",
        "    try:\n",
        "      image_data = (plt.imread(image_file,0).astype(float) - \n",
        "                    pixel_depth / 2) / pixel_depth\n",
        "      if image_data.shape != (image_size, image_size):\n",
        "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
        "      dataset[num_images, :, :] = image_data\n",
        "      num_images = num_images + 1\n",
        "    except IOError as e:\n",
        "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
        "    \n",
        "  dataset = dataset[0:num_images, :, :]\n",
        "  if num_images < min_num_images:\n",
        "    raise Exception('Many fewer images than expected: %d < %d' %\n",
        "                    (num_images, min_num_images))\n",
        "    \n",
        "  print('Full dataset tensor:', dataset.shape)\n",
        "  print('Mean:', np.mean(dataset))\n",
        "  print('Standard deviation:', np.std(dataset))\n",
        "  return dataset\n",
        "        \n",
        "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
        "  dataset_names = []\n",
        "  for folder in data_folders:\n",
        "    set_filename = folder + '.pickle'\n",
        "    dataset_names.append(set_filename)\n",
        "    if os.path.exists(set_filename) and not force:\n",
        "      # You may override by setting force=True.\n",
        "      print('%s already present - Skipping pickling.' % set_filename)\n",
        "    else:\n",
        "      print('Pickling %s.' % set_filename)\n",
        "      dataset = load_letter(folder, min_num_images_per_class)\n",
        "      try:\n",
        "        with open(set_filename, 'wb') as f:\n",
        "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
        "      except Exception as e:\n",
        "        print('Unable to save data to', set_filename, ':', e)\n",
        "  \n",
        "  return dataset_names\n",
        "\n",
        "train_datasets = maybe_pickle(train_folders, 1700)\n",
        "test_datasets = maybe_pickle(test_folders, 300)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train/character_11_taamatar.pickle already present - Skipping pickling.\n",
            "Train/character_13_daa.pickle already present - Skipping pickling.\n",
            "Train/character_16_tabala.pickle already present - Skipping pickling.\n",
            "Train/character_18_da.pickle already present - Skipping pickling.\n",
            "Train/character_1_ka.pickle already present - Skipping pickling.\n",
            "Train/character_21_pa.pickle already present - Skipping pickling.\n",
            "Train/character_25_ma.pickle already present - Skipping pickling.\n",
            "Train/character_26_yaw.pickle already present - Skipping pickling.\n",
            "Train/character_27_ra.pickle already present - Skipping pickling.\n",
            "Train/character_28_la.pickle already present - Skipping pickling.\n",
            "Train/character_29_waw.pickle already present - Skipping pickling.\n",
            "Train/character_30_motosaw.pickle already present - Skipping pickling.\n",
            "Train/character_32_patalosaw.pickle already present - Skipping pickling.\n",
            "Train/character_33_ha.pickle already present - Skipping pickling.\n",
            "Train/character_3_ga.pickle already present - Skipping pickling.\n",
            "Train/character_4_gha.pickle already present - Skipping pickling.\n",
            "Train/character_6_cha.pickle already present - Skipping pickling.\n",
            "Train/character_8_ja.pickle already present - Skipping pickling.\n",
            "Test/character_11_taamatar.pickle already present - Skipping pickling.\n",
            "Test/character_13_daa.pickle already present - Skipping pickling.\n",
            "Test/character_16_tabala.pickle already present - Skipping pickling.\n",
            "Test/character_18_da.pickle already present - Skipping pickling.\n",
            "Test/character_1_ka.pickle already present - Skipping pickling.\n",
            "Test/character_21_pa.pickle already present - Skipping pickling.\n",
            "Test/character_25_ma.pickle already present - Skipping pickling.\n",
            "Test/character_26_yaw.pickle already present - Skipping pickling.\n",
            "Test/character_27_ra.pickle already present - Skipping pickling.\n",
            "Test/character_28_la.pickle already present - Skipping pickling.\n",
            "Test/character_29_waw.pickle already present - Skipping pickling.\n",
            "Test/character_30_motosaw.pickle already present - Skipping pickling.\n",
            "Test/character_32_patalosaw.pickle already present - Skipping pickling.\n",
            "Test/character_33_ha.pickle already present - Skipping pickling.\n",
            "Test/character_3_ga.pickle already present - Skipping pickling.\n",
            "Test/character_4_gha.pickle already present - Skipping pickling.\n",
            "Test/character_6_cha.pickle already present - Skipping pickling.\n",
            "Test/character_8_ja.pickle already present - Skipping pickling.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJN1uY2yKCEa",
        "colab_type": "code",
        "outputId": "17966723-21b9-4260-9186-07ebcdc38f52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "pickle_file = train_datasets[0]  \n",
        "\n",
        "# With would automatically close the file after the nested block of code\n",
        "with open(pickle_file, 'rb') as f:\n",
        "    \n",
        "    # unpickle\n",
        "    letter_set = pickle.load(f)  \n",
        "    \n",
        "    # pick a random image index\n",
        "    sample_idx = np.random.randint(len(letter_set))\n",
        "    \n",
        "    # extract a 2D slice\n",
        "    sample_image = letter_set[sample_idx, :, :]  \n",
        "    plt.figure()\n",
        "    \n",
        "    # display it\n",
        "    plt.imshow(sample_image) "
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swvgWqi0KGM9",
        "colab_type": "code",
        "outputId": "9f55a6ef-98c9-4615-d7fc-9f0bacc91426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "def make_arrays(nb_rows, img_size):\n",
        "  if nb_rows:\n",
        "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
        "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
        "  else:\n",
        "    dataset, labels = None, None\n",
        "  return dataset, labels\n",
        "\n",
        "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
        "  num_classes = len(pickle_files)\n",
        "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
        "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
        "  vsize_per_class = valid_size // num_classes\n",
        "  tsize_per_class = train_size // num_classes\n",
        "    \n",
        "  start_v, start_t = 0, 0\n",
        "  end_v, end_t = vsize_per_class, tsize_per_class\n",
        "  end_l = vsize_per_class+tsize_per_class\n",
        "  for label, pickle_file in enumerate(pickle_files):       \n",
        "    try:\n",
        "      with open(pickle_file, 'rb') as f:\n",
        "        letter_set = pickle.load(f)\n",
        "        # let's shuffle the letters to have random validation and training set\n",
        "        np.random.shuffle(letter_set)\n",
        "        if valid_dataset is not None:\n",
        "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
        "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
        "          valid_labels[start_v:end_v] = label\n",
        "          start_v += vsize_per_class\n",
        "          end_v += vsize_per_class\n",
        "                    \n",
        "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
        "        train_dataset[start_t:end_t, :, :] = train_letter\n",
        "        train_labels[start_t:end_t] = label\n",
        "        start_t += tsize_per_class\n",
        "        end_t += tsize_per_class\n",
        "    except Exception as e:\n",
        "      print('Unable to process data from', pickle_file, ':', e)\n",
        "      raise\n",
        "    \n",
        "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
        "            \n",
        "            \n",
        "train_size = 30600\n",
        "valid_size = 0\n",
        "test_size = 5400\n",
        "\n",
        "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
        "  train_datasets, train_size, valid_size)\n",
        "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
        "trainloader=torch.utils.data.DataLoader(train_dataset,batch_size=18,shuffle=True)\n",
        "print('Training:', train_dataset.shape, train_labels.shape)\n",
        "print('Testing:', test_dataset.shape, test_labels.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training: (30600, 32, 32) (30600,)\n",
            "Testing: (5400, 32, 32) (5400,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFiwuPENKOlp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def randomize(dataset, labels):\n",
        "  permutation = np.random.permutation(labels.shape[0])\n",
        "  shuffled_dataset = dataset[permutation,:,:]\n",
        "  shuffled_labels = labels[permutation]\n",
        "  return shuffled_dataset, shuffled_labels\n",
        "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
        "test_dataset, test_labels = randomize(test_dataset, test_labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MomW_EWrKYWT",
        "colab_type": "code",
        "outputId": "75c532ea-ae86-49d7-9da7-6e66f09c4f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train_dataset.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30600, 32, 32)\n",
            "(30600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK7G-IeOKg3x",
        "colab_type": "code",
        "outputId": "17be7e00-c9e2-4800-df70-13c0c1f3d3c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.utils import np_utils\n",
        "train_labels= np_utils.to_categorical(train_labels,18)\n",
        "test_labels = np_utils.to_categorical(test_labels, 18)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8I307goKjgc",
        "colab_type": "code",
        "outputId": "c505a849-13d2-4212-d7a3-87ed56546a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_dataset = train_dataset.reshape(train_dataset.shape[0], 1,32, 32)\n",
        "test_dataset = test_dataset.reshape(test_dataset.shape[0], 1, 32, 32)\n",
        "print(train_dataset.shape)\n",
        "print(train_labels.shape)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30600, 1, 32, 32)\n",
            "(30600,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDzcjOHFg7rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKbNOx5CWPIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 18)\n",
        "        self.fc4=nn.LogSoftmax()\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # If the size is a square you can only specify a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x=self.fc4(x)\n",
        "       \n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "net = Net()\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAGHmKIxWRRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeTMeBIkcW3T",
        "colab_type": "code",
        "outputId": "bee2cba6-7e4d-4b1c-b8b4-5a9f93cc0af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "t_l = torch.FloatTensor(train_labels)\n",
        "t_d = torch.FloatTensor(train_dataset)\n",
        "print (train_dataset.shape)\n",
        "from torch.utils.data import TensorDataset\n",
        "train_ds = TensorDataset(t_d,t_l)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30600, 1, 32, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SBgoqQ0YzDK",
        "colab_type": "code",
        "outputId": "0f650a09-dd3b-4e69-f525-b58ac77faa03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_ds,0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.unsqueeze(0)\n",
        "        #inputs=inputs.reshape(1,1,32,32)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "\n",
        " \n",
        "        for k in range(18):\n",
        "          if (labels[k]==1):\n",
        "        #    print (k)\n",
        "            t=torch.tensor([k])\n",
        "\n",
        "        t=t.type(torch.LongTensor)\n",
        "        \n",
        "\n",
        "        loss = criterion(outputs, t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.891\n",
            "[1,  4000] loss: 2.490\n",
            "[1,  6000] loss: 1.391\n",
            "[1,  8000] loss: 0.920\n",
            "[1, 10000] loss: 0.642\n",
            "[1, 12000] loss: 0.507\n",
            "[1, 14000] loss: 0.448\n",
            "[1, 16000] loss: 0.382\n",
            "[1, 18000] loss: 0.342\n",
            "[1, 20000] loss: 0.337\n",
            "[1, 22000] loss: 0.294\n",
            "[1, 24000] loss: 0.282\n",
            "[1, 26000] loss: 0.277\n",
            "[1, 28000] loss: 0.276\n",
            "[1, 30000] loss: 0.250\n",
            "[2,  2000] loss: 0.197\n",
            "[2,  4000] loss: 0.214\n",
            "[2,  6000] loss: 0.172\n",
            "[2,  8000] loss: 0.185\n",
            "[2, 10000] loss: 0.161\n",
            "[2, 12000] loss: 0.176\n",
            "[2, 14000] loss: 0.182\n",
            "[2, 16000] loss: 0.135\n",
            "[2, 18000] loss: 0.123\n",
            "[2, 20000] loss: 0.143\n",
            "[2, 22000] loss: 0.119\n",
            "[2, 24000] loss: 0.144\n",
            "[2, 26000] loss: 0.131\n",
            "[2, 28000] loss: 0.146\n",
            "[2, 30000] loss: 0.097\n",
            "[3,  2000] loss: 0.100\n",
            "[3,  4000] loss: 0.106\n",
            "[3,  6000] loss: 0.082\n",
            "[3,  8000] loss: 0.108\n",
            "[3, 10000] loss: 0.083\n",
            "[3, 12000] loss: 0.117\n",
            "[3, 14000] loss: 0.100\n",
            "[3, 16000] loss: 0.098\n",
            "[3, 18000] loss: 0.073\n",
            "[3, 20000] loss: 0.099\n",
            "[3, 22000] loss: 0.071\n",
            "[3, 24000] loss: 0.084\n",
            "[3, 26000] loss: 0.101\n",
            "[3, 28000] loss: 0.118\n",
            "[3, 30000] loss: 0.087\n",
            "[4,  2000] loss: 0.096\n",
            "[4,  4000] loss: 0.074\n",
            "[4,  6000] loss: 0.081\n",
            "[4,  8000] loss: 0.077\n",
            "[4, 10000] loss: 0.077\n",
            "[4, 12000] loss: 0.073\n",
            "[4, 14000] loss: 0.081\n",
            "[4, 16000] loss: 0.051\n",
            "[4, 18000] loss: 0.054\n",
            "[4, 20000] loss: 0.094\n",
            "[4, 22000] loss: 0.068\n",
            "[4, 24000] loss: 0.042\n",
            "[4, 26000] loss: 0.107\n",
            "[4, 28000] loss: 0.077\n",
            "[4, 30000] loss: 0.043\n",
            "[5,  2000] loss: 0.082\n",
            "[5,  4000] loss: 0.066\n",
            "[5,  6000] loss: 0.037\n",
            "[5,  8000] loss: 0.058\n",
            "[5, 10000] loss: 0.050\n",
            "[5, 12000] loss: 0.055\n",
            "[5, 14000] loss: 0.063\n",
            "[5, 16000] loss: 0.042\n",
            "[5, 18000] loss: 0.062\n",
            "[5, 20000] loss: 0.067\n",
            "[5, 22000] loss: 0.060\n",
            "[5, 24000] loss: 0.057\n",
            "[5, 26000] loss: 0.067\n",
            "[5, 28000] loss: 0.056\n",
            "[5, 30000] loss: 0.066\n",
            "[6,  2000] loss: 0.035\n",
            "[6,  4000] loss: 0.050\n",
            "[6,  6000] loss: 0.048\n",
            "[6,  8000] loss: 0.067\n",
            "[6, 10000] loss: 0.070\n",
            "[6, 12000] loss: 0.049\n",
            "[6, 14000] loss: 0.091\n",
            "[6, 16000] loss: 0.054\n",
            "[6, 18000] loss: 0.062\n",
            "[6, 20000] loss: 0.043\n",
            "[6, 22000] loss: 0.057\n",
            "[6, 24000] loss: 0.058\n",
            "[6, 26000] loss: 0.048\n",
            "[6, 28000] loss: 0.077\n",
            "[6, 30000] loss: 0.055\n",
            "[7,  2000] loss: 0.035\n",
            "[7,  4000] loss: 0.058\n",
            "[7,  6000] loss: 0.029\n",
            "[7,  8000] loss: 0.051\n",
            "[7, 10000] loss: 0.037\n",
            "[7, 12000] loss: 0.082\n",
            "[7, 14000] loss: 0.083\n",
            "[7, 16000] loss: 0.091\n",
            "[7, 18000] loss: 0.032\n",
            "[7, 20000] loss: 0.052\n",
            "[7, 22000] loss: 0.034\n",
            "[7, 24000] loss: 0.053\n",
            "[7, 26000] loss: 0.044\n",
            "[7, 28000] loss: 0.086\n",
            "[7, 30000] loss: 0.037\n",
            "[8,  2000] loss: 0.087\n",
            "[8,  4000] loss: 0.069\n",
            "[8,  6000] loss: 0.032\n",
            "[8,  8000] loss: 0.041\n",
            "[8, 10000] loss: 0.015\n",
            "[8, 12000] loss: 0.025\n",
            "[8, 14000] loss: 0.056\n",
            "[8, 16000] loss: 0.041\n",
            "[8, 18000] loss: 0.039\n",
            "[8, 20000] loss: 0.043\n",
            "[8, 22000] loss: 0.052\n",
            "[8, 24000] loss: 0.048\n",
            "[8, 26000] loss: 0.029\n",
            "[8, 28000] loss: 0.045\n",
            "[8, 30000] loss: 0.057\n",
            "[9,  2000] loss: 0.072\n",
            "[9,  4000] loss: 0.045\n",
            "[9,  6000] loss: 0.049\n",
            "[9,  8000] loss: 0.046\n",
            "[9, 10000] loss: 0.070\n",
            "[9, 12000] loss: 0.043\n",
            "[9, 14000] loss: 0.058\n",
            "[9, 16000] loss: 0.037\n",
            "[9, 18000] loss: 0.058\n",
            "[9, 20000] loss: 0.070\n",
            "[9, 22000] loss: 0.040\n",
            "[9, 24000] loss: 0.040\n",
            "[9, 26000] loss: 0.069\n",
            "[9, 28000] loss: 0.052\n",
            "[9, 30000] loss: 0.039\n",
            "[10,  2000] loss: 0.047\n",
            "[10,  4000] loss: 0.055\n",
            "[10,  6000] loss: 0.042\n",
            "[10,  8000] loss: 0.057\n",
            "[10, 10000] loss: 0.058\n",
            "[10, 12000] loss: 0.068\n",
            "[10, 14000] loss: 0.048\n",
            "[10, 16000] loss: 0.102\n",
            "[10, 18000] loss: 0.064\n",
            "[10, 20000] loss: 0.034\n",
            "[10, 22000] loss: 0.047\n",
            "[10, 24000] loss: 0.018\n",
            "[10, 26000] loss: 0.036\n",
            "[10, 28000] loss: 0.052\n",
            "[10, 30000] loss: 0.035\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTfeJZr8Znf1",
        "colab_type": "code",
        "outputId": "3dc1c43d-a5ff-43ed-c7b1-474092ea5f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "test_l = torch.FloatTensor(test_labels)\n",
        "test_d = torch.FloatTensor(test_dataset)\n",
        "print (test_dataset.shape)\n",
        "\n",
        "test_ds = TensorDataset(test_d,test_l)\n",
        "correct = 0\n",
        "\n",
        "for i, data in enumerate(test_ds, 0):\n",
        "  test_x, test_y = data\n",
        "  test_x=test_x.unsqueeze(0)\n",
        "  pred = net.forward(test_x)\n",
        "  for k in range(18):\n",
        "    if(test_y[k]==1):\n",
        "      t=torch.tensor([k])\n",
        "  t=t.type(torch.LongTensor)\n",
        "  y_hat = np.argmax(pred.data)\n",
        " # print (y_hat)\n",
        "#  print(t)\n",
        "  if y_hat == t:\n",
        "    correct += 1\n",
        "\n",
        "print(\"Accuracy={}\".format(correct / len(test_dataset)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5400, 1, 32, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy=0.9690740740740741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSqChvsyh04E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1078fa67-b55e-4b16-8620-156490a7ef9e"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "print (train_labels)\n",
        "for index, (train_indices, test_indices) in enumerate(skf.split(train_dataset, train_labels)):\n",
        "  \n",
        "  print(\"Training on fold \" + str(index+1) + \"/10...\")\n",
        "  \n",
        "  xtrain, xtest = train_dataset[train_indices], train_dataset[test_indices]\n",
        "  ytrain, ytest = train_labels[train_indices], train_labels[test_indices]\n",
        "  \n",
        "  ytrain = np_utils.to_categorical(ytrain, 18)\n",
        "  ytest = np_utils.to_categorical(ytest, 18)\n",
        "  \n",
        "  net1=Net()\n",
        "  criterion1 = nn.CrossEntropyLoss()\n",
        "  optimizer1 = optim.SGD(net1.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "  t_l = torch.FloatTensor(ytrain)\n",
        "  t_d = torch.FloatTensor(xtrain)\n",
        "  train_ds = TensorDataset(t_d,t_l)\n",
        "  for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_ds,0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs=inputs.unsqueeze(0)\n",
        "        #inputs=inputs.reshape(1,1,32,32)\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "\n",
        " \n",
        "        for k in range(18):\n",
        "          if (labels[k]==1):\n",
        "        #    print (k)\n",
        "            t=torch.tensor([k])\n",
        "\n",
        "        t=t.type(torch.LongTensor)\n",
        "        \n",
        "\n",
        "        loss = criterion(outputs, t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "  print('Finished Training')\n",
        "  \n",
        "  print(\"Training new iteration on \" + str(xtrain.shape[0]) + \" training samples, \" + str(xtest.shape[0]) + \" validation samples\")\n",
        "  test_l = torch.FloatTensor(ytest)\n",
        "  test_d = torch.FloatTensor(xtest)\n",
        "  print (test_dataset.shape)\n",
        "\n",
        "  test_ds = TensorDataset(test_d,test_l)\n",
        "  correct = 0\n",
        "\n",
        "  for i, data in enumerate(test_ds, 0):\n",
        "    test_x, test_y = data\n",
        "    test_x=test_x.unsqueeze(0)\n",
        "    pred = net.forward(test_x)\n",
        "    for k in range(18):\n",
        "      if(test_y[k]==1):\n",
        "        t=torch.tensor([k])\n",
        "    t=t.type(torch.LongTensor)\n",
        "    y_hat = np.argmax(pred.data)\n",
        "    if y_hat == t:\n",
        "      correct += 1\n",
        "\n",
        "  print(\"Accuracy={}\".format(correct / len(test_dataset)))\n",
        "  "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 6 13 12 ...  7  9  8]\n",
            "Training on fold 1/10...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.884\n",
            "[1,  4000] loss: 2.227\n",
            "[1,  6000] loss: 1.232\n",
            "[1,  8000] loss: 0.796\n",
            "[1, 10000] loss: 0.637\n",
            "[1, 12000] loss: 0.511\n",
            "[1, 14000] loss: 0.428\n",
            "[1, 16000] loss: 0.410\n",
            "[1, 18000] loss: 0.366\n",
            "[1, 20000] loss: 0.309\n",
            "[1, 22000] loss: 0.293\n",
            "[1, 24000] loss: 0.267\n",
            "[1, 26000] loss: 0.276\n",
            "[2,  2000] loss: 0.273\n",
            "[2,  4000] loss: 0.221\n",
            "[2,  6000] loss: 0.242\n",
            "[2,  8000] loss: 0.179\n",
            "[2, 10000] loss: 0.185\n",
            "[2, 12000] loss: 0.181\n",
            "[2, 14000] loss: 0.143\n",
            "[2, 16000] loss: 0.141\n",
            "[2, 18000] loss: 0.139\n",
            "[2, 20000] loss: 0.107\n",
            "[2, 22000] loss: 0.134\n",
            "[2, 24000] loss: 0.133\n",
            "[2, 26000] loss: 0.125\n",
            "[3,  2000] loss: 0.149\n",
            "[3,  4000] loss: 0.126\n",
            "[3,  6000] loss: 0.124\n",
            "[3,  8000] loss: 0.109\n",
            "[3, 10000] loss: 0.110\n",
            "[3, 12000] loss: 0.115\n",
            "[3, 14000] loss: 0.101\n",
            "[3, 16000] loss: 0.106\n",
            "[3, 18000] loss: 0.068\n",
            "[3, 20000] loss: 0.065\n",
            "[3, 22000] loss: 0.085\n",
            "[3, 24000] loss: 0.103\n",
            "[3, 26000] loss: 0.079\n",
            "[4,  2000] loss: 0.072\n",
            "[4,  4000] loss: 0.078\n",
            "[4,  6000] loss: 0.106\n",
            "[4,  8000] loss: 0.079\n",
            "[4, 10000] loss: 0.092\n",
            "[4, 12000] loss: 0.092\n",
            "[4, 14000] loss: 0.085\n",
            "[4, 16000] loss: 0.068\n",
            "[4, 18000] loss: 0.077\n",
            "[4, 20000] loss: 0.058\n",
            "[4, 22000] loss: 0.060\n",
            "[4, 24000] loss: 0.070\n",
            "[4, 26000] loss: 0.094\n",
            "[5,  2000] loss: 0.092\n",
            "[5,  4000] loss: 0.079\n",
            "[5,  6000] loss: 0.073\n",
            "[5,  8000] loss: 0.062\n",
            "[5, 10000] loss: 0.096\n",
            "[5, 12000] loss: 0.082\n",
            "[5, 14000] loss: 0.070\n",
            "[5, 16000] loss: 0.058\n",
            "[5, 18000] loss: 0.104\n",
            "[5, 20000] loss: 0.059\n",
            "[5, 22000] loss: 0.080\n",
            "[5, 24000] loss: 0.057\n",
            "[5, 26000] loss: 0.074\n",
            "[6,  2000] loss: 0.087\n",
            "[6,  4000] loss: 0.048\n",
            "[6,  6000] loss: 0.048\n",
            "[6,  8000] loss: 0.070\n",
            "[6, 10000] loss: 0.055\n",
            "[6, 12000] loss: 0.074\n",
            "[6, 14000] loss: 0.058\n",
            "[6, 16000] loss: 0.041\n",
            "[6, 18000] loss: 0.040\n",
            "[6, 20000] loss: 0.062\n",
            "[6, 22000] loss: 0.093\n",
            "[6, 24000] loss: 0.037\n",
            "[6, 26000] loss: 0.073\n",
            "[7,  2000] loss: 0.072\n",
            "[7,  4000] loss: 0.071\n",
            "[7,  6000] loss: 0.066\n",
            "[7,  8000] loss: 0.054\n",
            "[7, 10000] loss: 0.056\n",
            "[7, 12000] loss: 0.036\n",
            "[7, 14000] loss: 0.059\n",
            "[7, 16000] loss: 0.072\n",
            "[7, 18000] loss: 0.051\n",
            "[7, 20000] loss: 0.078\n",
            "[7, 22000] loss: 0.040\n",
            "[7, 24000] loss: 0.029\n",
            "[7, 26000] loss: 0.040\n",
            "[8,  2000] loss: 0.048\n",
            "[8,  4000] loss: 0.051\n",
            "[8,  6000] loss: 0.071\n",
            "[8,  8000] loss: 0.081\n",
            "[8, 10000] loss: 0.033\n",
            "[8, 12000] loss: 0.064\n",
            "[8, 14000] loss: 0.053\n",
            "[8, 16000] loss: 0.025\n",
            "[8, 18000] loss: 0.041\n",
            "[8, 20000] loss: 0.089\n",
            "[8, 22000] loss: 0.063\n",
            "[8, 24000] loss: 0.049\n",
            "[8, 26000] loss: 0.085\n",
            "[9,  2000] loss: 0.049\n",
            "[9,  4000] loss: 0.076\n",
            "[9,  6000] loss: 0.057\n",
            "[9,  8000] loss: 0.040\n",
            "[9, 10000] loss: 0.053\n",
            "[9, 12000] loss: 0.030\n",
            "[9, 14000] loss: 0.043\n",
            "[9, 16000] loss: 0.072\n",
            "[9, 18000] loss: 0.028\n",
            "[9, 20000] loss: 0.046\n",
            "[9, 22000] loss: 0.052\n",
            "[9, 24000] loss: 0.028\n",
            "[9, 26000] loss: 0.056\n",
            "[10,  2000] loss: 0.020\n",
            "[10,  4000] loss: 0.037\n",
            "[10,  6000] loss: 0.027\n",
            "[10,  8000] loss: 0.012\n",
            "[10, 10000] loss: 0.069\n",
            "[10, 12000] loss: 0.061\n",
            "[10, 14000] loss: 0.028\n",
            "[10, 16000] loss: 0.036\n",
            "[10, 18000] loss: 0.065\n",
            "[10, 20000] loss: 0.033\n",
            "[10, 22000] loss: 0.034\n",
            "[10, 24000] loss: 0.031\n",
            "[10, 26000] loss: 0.032\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.5494444444444444\n",
            "Training on fold 2/10...\n",
            "[1,  2000] loss: 0.034\n",
            "[1,  4000] loss: 0.085\n",
            "[1,  6000] loss: 0.095\n",
            "[1,  8000] loss: 0.076\n",
            "[1, 10000] loss: 0.043\n",
            "[1, 12000] loss: 0.055\n",
            "[1, 14000] loss: 0.084\n",
            "[1, 16000] loss: 0.066\n",
            "[1, 18000] loss: 0.050\n",
            "[1, 20000] loss: 0.040\n",
            "[1, 22000] loss: 0.056\n",
            "[1, 24000] loss: 0.028\n",
            "[1, 26000] loss: 0.028\n",
            "[2,  2000] loss: 0.048\n",
            "[2,  4000] loss: 0.032\n",
            "[2,  6000] loss: 0.064\n",
            "[2,  8000] loss: 0.066\n",
            "[2, 10000] loss: 0.079\n",
            "[2, 12000] loss: 0.050\n",
            "[2, 14000] loss: 0.066\n",
            "[2, 16000] loss: 0.047\n",
            "[2, 18000] loss: 0.027\n",
            "[2, 20000] loss: 0.023\n",
            "[2, 22000] loss: 0.036\n",
            "[2, 24000] loss: 0.049\n",
            "[2, 26000] loss: 0.034\n",
            "[3,  2000] loss: 0.059\n",
            "[3,  4000] loss: 0.041\n",
            "[3,  6000] loss: 0.009\n",
            "[3,  8000] loss: 0.015\n",
            "[3, 10000] loss: 0.040\n",
            "[3, 12000] loss: 0.034\n",
            "[3, 14000] loss: 0.020\n",
            "[3, 16000] loss: 0.041\n",
            "[3, 18000] loss: 0.021\n",
            "[3, 20000] loss: 0.031\n",
            "[3, 22000] loss: 0.043\n",
            "[3, 24000] loss: 0.067\n",
            "[3, 26000] loss: 0.042\n",
            "[4,  2000] loss: 0.029\n",
            "[4,  4000] loss: 0.073\n",
            "[4,  6000] loss: 0.035\n",
            "[4,  8000] loss: 0.046\n",
            "[4, 10000] loss: 0.040\n",
            "[4, 12000] loss: 0.049\n",
            "[4, 14000] loss: 0.062\n",
            "[4, 16000] loss: 0.073\n",
            "[4, 18000] loss: 0.023\n",
            "[4, 20000] loss: 0.025\n",
            "[4, 22000] loss: 0.041\n",
            "[4, 24000] loss: 0.050\n",
            "[4, 26000] loss: 0.047\n",
            "[5,  2000] loss: 0.068\n",
            "[5,  4000] loss: 0.054\n",
            "[5,  6000] loss: 0.044\n",
            "[5,  8000] loss: 0.041\n",
            "[5, 10000] loss: 0.044\n",
            "[5, 12000] loss: 0.029\n",
            "[5, 14000] loss: 0.080\n",
            "[5, 16000] loss: 0.051\n",
            "[5, 18000] loss: 0.044\n",
            "[5, 20000] loss: 0.036\n",
            "[5, 22000] loss: 0.091\n",
            "[5, 24000] loss: 0.039\n",
            "[5, 26000] loss: 0.042\n",
            "[6,  2000] loss: 0.090\n",
            "[6,  4000] loss: 0.051\n",
            "[6,  6000] loss: 0.055\n",
            "[6,  8000] loss: 0.074\n",
            "[6, 10000] loss: 0.108\n",
            "[6, 12000] loss: 0.075\n",
            "[6, 14000] loss: 0.057\n",
            "[6, 16000] loss: 0.040\n",
            "[6, 18000] loss: 0.047\n",
            "[6, 20000] loss: 0.024\n",
            "[6, 22000] loss: 0.060\n",
            "[6, 24000] loss: 0.049\n",
            "[6, 26000] loss: 0.050\n",
            "[7,  2000] loss: 0.091\n",
            "[7,  4000] loss: 0.048\n",
            "[7,  6000] loss: 0.042\n",
            "[7,  8000] loss: 0.061\n",
            "[7, 10000] loss: 0.041\n",
            "[7, 12000] loss: 0.049\n",
            "[7, 14000] loss: 0.019\n",
            "[7, 16000] loss: 0.035\n",
            "[7, 18000] loss: 0.051\n",
            "[7, 20000] loss: 0.025\n",
            "[7, 22000] loss: 0.026\n",
            "[7, 24000] loss: 0.013\n",
            "[7, 26000] loss: 0.059\n",
            "[8,  2000] loss: 0.021\n",
            "[8,  4000] loss: 0.016\n",
            "[8,  6000] loss: 0.056\n",
            "[8,  8000] loss: 0.021\n",
            "[8, 10000] loss: 0.025\n",
            "[8, 12000] loss: 0.029\n",
            "[8, 14000] loss: 0.050\n",
            "[8, 16000] loss: 0.025\n",
            "[8, 18000] loss: 0.062\n",
            "[8, 20000] loss: 0.053\n",
            "[8, 22000] loss: 0.104\n",
            "[8, 24000] loss: 0.086\n",
            "[8, 26000] loss: 0.028\n",
            "[9,  2000] loss: 0.083\n",
            "[9,  4000] loss: 0.071\n",
            "[9,  6000] loss: 0.047\n",
            "[9,  8000] loss: 0.058\n",
            "[9, 10000] loss: 0.061\n",
            "[9, 12000] loss: 0.054\n",
            "[9, 14000] loss: 0.043\n",
            "[9, 16000] loss: 0.027\n",
            "[9, 18000] loss: 0.031\n",
            "[9, 20000] loss: 0.029\n",
            "[9, 22000] loss: 0.019\n",
            "[9, 24000] loss: 0.032\n",
            "[9, 26000] loss: 0.027\n",
            "[10,  2000] loss: 0.052\n",
            "[10,  4000] loss: 0.035\n",
            "[10,  6000] loss: 0.021\n",
            "[10,  8000] loss: 0.012\n",
            "[10, 10000] loss: 0.025\n",
            "[10, 12000] loss: 0.028\n",
            "[10, 14000] loss: 0.042\n",
            "[10, 16000] loss: 0.035\n",
            "[10, 18000] loss: 0.076\n",
            "[10, 20000] loss: 0.073\n",
            "[10, 22000] loss: 0.028\n",
            "[10, 24000] loss: 0.040\n",
            "[10, 26000] loss: 0.045\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.5516666666666666\n",
            "Training on fold 3/10...\n",
            "[1,  2000] loss: 0.069\n",
            "[1,  4000] loss: 0.131\n",
            "[1,  6000] loss: 0.085\n",
            "[1,  8000] loss: 0.046\n",
            "[1, 10000] loss: 0.033\n",
            "[1, 12000] loss: 0.045\n",
            "[1, 14000] loss: 0.036\n",
            "[1, 16000] loss: 0.061\n",
            "[1, 18000] loss: 0.088\n",
            "[1, 20000] loss: 0.089\n",
            "[1, 22000] loss: 0.056\n",
            "[1, 24000] loss: 0.041\n",
            "[1, 26000] loss: 0.075\n",
            "[2,  2000] loss: 0.132\n",
            "[2,  4000] loss: 0.056\n",
            "[2,  6000] loss: 0.039\n",
            "[2,  8000] loss: 0.021\n",
            "[2, 10000] loss: 0.074\n",
            "[2, 12000] loss: 0.057\n",
            "[2, 14000] loss: 0.025\n",
            "[2, 16000] loss: 0.037\n",
            "[2, 18000] loss: 0.029\n",
            "[2, 20000] loss: 0.070\n",
            "[2, 22000] loss: 0.028\n",
            "[2, 24000] loss: 0.066\n",
            "[2, 26000] loss: 0.045\n",
            "[3,  2000] loss: 0.051\n",
            "[3,  4000] loss: 0.034\n",
            "[3,  6000] loss: 0.039\n",
            "[3,  8000] loss: 0.054\n",
            "[3, 10000] loss: 0.048\n",
            "[3, 12000] loss: 0.077\n",
            "[3, 14000] loss: 0.047\n",
            "[3, 16000] loss: 0.037\n",
            "[3, 18000] loss: 0.067\n",
            "[3, 20000] loss: 0.083\n",
            "[3, 22000] loss: 0.033\n",
            "[3, 24000] loss: 0.037\n",
            "[3, 26000] loss: 0.032\n",
            "[4,  2000] loss: 0.055\n",
            "[4,  4000] loss: 0.025\n",
            "[4,  6000] loss: 0.043\n",
            "[4,  8000] loss: 0.037\n",
            "[4, 10000] loss: 0.082\n",
            "[4, 12000] loss: 0.061\n",
            "[4, 14000] loss: 0.058\n",
            "[4, 16000] loss: 0.125\n",
            "[4, 18000] loss: 0.120\n",
            "[4, 20000] loss: 0.060\n",
            "[4, 22000] loss: 0.041\n",
            "[4, 24000] loss: 0.029\n",
            "[4, 26000] loss: 0.029\n",
            "[5,  2000] loss: 0.053\n",
            "[5,  4000] loss: 0.023\n",
            "[5,  6000] loss: 0.070\n",
            "[5,  8000] loss: 0.057\n",
            "[5, 10000] loss: 0.074\n",
            "[5, 12000] loss: 0.097\n",
            "[5, 14000] loss: 0.056\n",
            "[5, 16000] loss: 0.050\n",
            "[5, 18000] loss: 0.023\n",
            "[5, 20000] loss: 0.022\n",
            "[5, 22000] loss: 0.052\n",
            "[5, 24000] loss: 0.145\n",
            "[5, 26000] loss: 0.068\n",
            "[6,  2000] loss: 0.091\n",
            "[6,  4000] loss: 0.102\n",
            "[6,  6000] loss: 0.099\n",
            "[6,  8000] loss: 0.056\n",
            "[6, 10000] loss: 0.054\n",
            "[6, 12000] loss: 0.049\n",
            "[6, 14000] loss: 0.061\n",
            "[6, 16000] loss: 0.064\n",
            "[6, 18000] loss: 0.085\n",
            "[6, 20000] loss: 0.076\n",
            "[6, 22000] loss: 0.080\n",
            "[6, 24000] loss: 0.066\n",
            "[6, 26000] loss: 0.078\n",
            "[7,  2000] loss: 0.131\n",
            "[7,  4000] loss: 0.121\n",
            "[7,  6000] loss: 0.074\n",
            "[7,  8000] loss: 0.082\n",
            "[7, 10000] loss: 0.033\n",
            "[7, 12000] loss: 0.070\n",
            "[7, 14000] loss: 0.129\n",
            "[7, 16000] loss: 0.098\n",
            "[7, 18000] loss: 0.059\n",
            "[7, 20000] loss: 0.048\n",
            "[7, 22000] loss: 0.034\n",
            "[7, 24000] loss: 0.076\n",
            "[7, 26000] loss: 0.071\n",
            "[8,  2000] loss: 0.115\n",
            "[8,  4000] loss: 0.117\n",
            "[8,  6000] loss: 0.133\n",
            "[8,  8000] loss: 0.112\n",
            "[8, 10000] loss: 0.100\n",
            "[8, 12000] loss: 0.099\n",
            "[8, 14000] loss: 0.047\n",
            "[8, 16000] loss: 0.044\n",
            "[8, 18000] loss: 0.062\n",
            "[8, 20000] loss: 0.076\n",
            "[8, 22000] loss: 0.090\n",
            "[8, 24000] loss: 0.049\n",
            "[8, 26000] loss: 0.054\n",
            "[9,  2000] loss: 0.135\n",
            "[9,  4000] loss: 0.107\n",
            "[9,  6000] loss: 0.163\n",
            "[9,  8000] loss: 0.325\n",
            "[9, 10000] loss: 0.104\n",
            "[9, 12000] loss: 0.133\n",
            "[9, 14000] loss: 0.059\n",
            "[9, 16000] loss: 0.156\n",
            "[9, 18000] loss: 0.111\n",
            "[9, 20000] loss: 0.058\n",
            "[9, 22000] loss: 0.108\n",
            "[9, 24000] loss: 0.092\n",
            "[9, 26000] loss: 0.073\n",
            "[10,  2000] loss: 0.111\n",
            "[10,  4000] loss: 0.057\n",
            "[10,  6000] loss: 0.121\n",
            "[10,  8000] loss: 0.093\n",
            "[10, 10000] loss: 0.126\n",
            "[10, 12000] loss: 0.064\n",
            "[10, 14000] loss: 0.040\n",
            "[10, 16000] loss: 0.062\n",
            "[10, 18000] loss: 0.065\n",
            "[10, 20000] loss: 0.090\n",
            "[10, 22000] loss: 0.064\n",
            "[10, 24000] loss: 0.038\n",
            "[10, 26000] loss: 0.047\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.545\n",
            "Training on fold 4/10...\n",
            "[1,  2000] loss: 0.123\n",
            "[1,  4000] loss: 0.053\n",
            "[1,  6000] loss: 0.071\n",
            "[1,  8000] loss: 0.110\n",
            "[1, 10000] loss: 0.060\n",
            "[1, 12000] loss: 0.081\n",
            "[1, 14000] loss: 0.083\n",
            "[1, 16000] loss: 0.050\n",
            "[1, 18000] loss: 0.189\n",
            "[1, 20000] loss: 0.263\n",
            "[1, 22000] loss: 0.149\n",
            "[1, 24000] loss: 0.141\n",
            "[1, 26000] loss: 0.060\n",
            "[2,  2000] loss: 0.077\n",
            "[2,  4000] loss: 0.053\n",
            "[2,  6000] loss: 0.038\n",
            "[2,  8000] loss: 0.070\n",
            "[2, 10000] loss: 0.079\n",
            "[2, 12000] loss: 0.123\n",
            "[2, 14000] loss: 0.096\n",
            "[2, 16000] loss: 0.069\n",
            "[2, 18000] loss: 0.053\n",
            "[2, 20000] loss: 0.039\n",
            "[2, 22000] loss: 0.048\n",
            "[2, 24000] loss: 0.050\n",
            "[2, 26000] loss: 0.051\n",
            "[3,  2000] loss: 0.052\n",
            "[3,  4000] loss: 0.058\n",
            "[3,  6000] loss: 0.041\n",
            "[3,  8000] loss: 0.022\n",
            "[3, 10000] loss: 0.058\n",
            "[3, 12000] loss: 0.054\n",
            "[3, 14000] loss: 0.068\n",
            "[3, 16000] loss: 0.054\n",
            "[3, 18000] loss: 0.086\n",
            "[3, 20000] loss: 0.065\n",
            "[3, 22000] loss: 0.138\n",
            "[3, 24000] loss: 0.084\n",
            "[3, 26000] loss: 0.040\n",
            "[4,  2000] loss: 0.055\n",
            "[4,  4000] loss: 0.054\n",
            "[4,  6000] loss: 0.062\n",
            "[4,  8000] loss: 0.082\n",
            "[4, 10000] loss: 0.070\n",
            "[4, 12000] loss: 0.069\n",
            "[4, 14000] loss: 0.063\n",
            "[4, 16000] loss: 0.048\n",
            "[4, 18000] loss: 0.084\n",
            "[4, 20000] loss: 0.029\n",
            "[4, 22000] loss: 0.049\n",
            "[4, 24000] loss: 0.029\n",
            "[4, 26000] loss: 0.053\n",
            "[5,  2000] loss: 0.068\n",
            "[5,  4000] loss: 0.065\n",
            "[5,  6000] loss: 0.057\n",
            "[5,  8000] loss: 0.073\n",
            "[5, 10000] loss: 0.086\n",
            "[5, 12000] loss: 0.084\n",
            "[5, 14000] loss: 0.042\n",
            "[5, 16000] loss: 0.032\n",
            "[5, 18000] loss: 0.014\n",
            "[5, 20000] loss: 0.018\n",
            "[5, 22000] loss: 0.024\n",
            "[5, 24000] loss: 0.021\n",
            "[5, 26000] loss: 0.040\n",
            "[6,  2000] loss: 0.019\n",
            "[6,  4000] loss: 0.042\n",
            "[6,  6000] loss: 0.042\n",
            "[6,  8000] loss: 0.032\n",
            "[6, 10000] loss: 0.036\n",
            "[6, 12000] loss: 0.089\n",
            "[6, 14000] loss: 0.099\n",
            "[6, 16000] loss: 0.107\n",
            "[6, 18000] loss: 0.131\n",
            "[6, 20000] loss: 0.060\n",
            "[6, 22000] loss: 0.071\n",
            "[6, 24000] loss: 0.068\n",
            "[6, 26000] loss: 0.149\n",
            "[7,  2000] loss: 0.127\n",
            "[7,  4000] loss: 0.120\n",
            "[7,  6000] loss: 0.095\n",
            "[7,  8000] loss: 0.115\n",
            "[7, 10000] loss: 0.076\n",
            "[7, 12000] loss: 0.069\n",
            "[7, 14000] loss: 0.094\n",
            "[7, 16000] loss: 0.123\n",
            "[7, 18000] loss: 0.208\n",
            "[7, 20000] loss: 0.143\n",
            "[7, 22000] loss: 0.076\n",
            "[7, 24000] loss: 0.096\n",
            "[7, 26000] loss: 0.054\n",
            "[8,  2000] loss: 0.187\n",
            "[8,  4000] loss: 0.255\n",
            "[8,  6000] loss: 0.134\n",
            "[8,  8000] loss: 0.056\n",
            "[8, 10000] loss: 0.145\n",
            "[8, 12000] loss: 0.078\n",
            "[8, 14000] loss: 0.086\n",
            "[8, 16000] loss: 0.105\n",
            "[8, 18000] loss: 0.130\n",
            "[8, 20000] loss: 0.111\n",
            "[8, 22000] loss: 0.080\n",
            "[8, 24000] loss: 0.069\n",
            "[8, 26000] loss: 0.098\n",
            "[9,  2000] loss: 0.100\n",
            "[9,  4000] loss: 0.132\n",
            "[9,  6000] loss: 0.215\n",
            "[9,  8000] loss: 0.096\n",
            "[9, 10000] loss: 0.137\n",
            "[9, 12000] loss: 0.121\n",
            "[9, 14000] loss: 0.079\n",
            "[9, 16000] loss: 0.058\n",
            "[9, 18000] loss: 0.084\n",
            "[9, 20000] loss: 0.117\n",
            "[9, 22000] loss: 0.118\n",
            "[9, 24000] loss: 0.103\n",
            "[9, 26000] loss: 0.104\n",
            "[10,  2000] loss: 0.033\n",
            "[10,  4000] loss: 0.109\n",
            "[10,  6000] loss: 0.072\n",
            "[10,  8000] loss: 0.053\n",
            "[10, 10000] loss: 0.076\n",
            "[10, 12000] loss: 0.084\n",
            "[10, 14000] loss: 0.174\n",
            "[10, 16000] loss: 0.118\n",
            "[10, 18000] loss: 0.107\n",
            "[10, 20000] loss: 0.069\n",
            "[10, 22000] loss: 0.068\n",
            "[10, 24000] loss: 0.133\n",
            "[10, 26000] loss: 0.581\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.5370370370370371\n",
            "Training on fold 5/10...\n",
            "[1,  2000] loss: 0.283\n",
            "[1,  4000] loss: 0.114\n",
            "[1,  6000] loss: 0.260\n",
            "[1,  8000] loss: 0.097\n",
            "[1, 10000] loss: 0.147\n",
            "[1, 12000] loss: 0.130\n",
            "[1, 14000] loss: 0.089\n",
            "[1, 16000] loss: 0.102\n",
            "[1, 18000] loss: 0.163\n",
            "[1, 20000] loss: 0.137\n",
            "[1, 22000] loss: 0.061\n",
            "[1, 24000] loss: 0.120\n",
            "[1, 26000] loss: 0.203\n",
            "[2,  2000] loss: 0.105\n",
            "[2,  4000] loss: 0.079\n",
            "[2,  6000] loss: 0.183\n",
            "[2,  8000] loss: 0.092\n",
            "[2, 10000] loss: 0.185\n",
            "[2, 12000] loss: 0.085\n",
            "[2, 14000] loss: 0.061\n",
            "[2, 16000] loss: 0.154\n",
            "[2, 18000] loss: 0.115\n",
            "[2, 20000] loss: 0.145\n",
            "[2, 22000] loss: 0.233\n",
            "[2, 24000] loss: 0.108\n",
            "[2, 26000] loss: 0.126\n",
            "[3,  2000] loss: 0.213\n",
            "[3,  4000] loss: 0.063\n",
            "[3,  6000] loss: 0.127\n",
            "[3,  8000] loss: 0.204\n",
            "[3, 10000] loss: 0.199\n",
            "[3, 12000] loss: 0.282\n",
            "[3, 14000] loss: 0.128\n",
            "[3, 16000] loss: 0.130\n",
            "[3, 18000] loss: 0.122\n",
            "[3, 20000] loss: 0.136\n",
            "[3, 22000] loss: 0.062\n",
            "[3, 24000] loss: 0.172\n",
            "[3, 26000] loss: 0.216\n",
            "[4,  2000] loss: 0.179\n",
            "[4,  4000] loss: 0.208\n",
            "[4,  6000] loss: 0.116\n",
            "[4,  8000] loss: 0.097\n",
            "[4, 10000] loss: 0.070\n",
            "[4, 12000] loss: 0.083\n",
            "[4, 14000] loss: 0.153\n",
            "[4, 16000] loss: 0.202\n",
            "[4, 18000] loss: 0.111\n",
            "[4, 20000] loss: 0.249\n",
            "[4, 22000] loss: 0.222\n",
            "[4, 24000] loss: 0.168\n",
            "[4, 26000] loss: 0.089\n",
            "[5,  2000] loss: 0.129\n",
            "[5,  4000] loss: 0.054\n",
            "[5,  6000] loss: 0.051\n",
            "[5,  8000] loss: 0.142\n",
            "[5, 10000] loss: 0.065\n",
            "[5, 12000] loss: 0.096\n",
            "[5, 14000] loss: 0.205\n",
            "[5, 16000] loss: 0.114\n",
            "[5, 18000] loss: 0.066\n",
            "[5, 20000] loss: 0.096\n",
            "[5, 22000] loss: 0.150\n",
            "[5, 24000] loss: 0.178\n",
            "[5, 26000] loss: 0.246\n",
            "[6,  2000] loss: 0.183\n",
            "[6,  4000] loss: 0.141\n",
            "[6,  6000] loss: 0.084\n",
            "[6,  8000] loss: 0.106\n",
            "[6, 10000] loss: 0.177\n",
            "[6, 12000] loss: 0.146\n",
            "[6, 14000] loss: 0.287\n",
            "[6, 16000] loss: 0.314\n",
            "[6, 18000] loss: 0.227\n",
            "[6, 20000] loss: 0.122\n",
            "[6, 22000] loss: 0.303\n",
            "[6, 24000] loss: 0.224\n",
            "[6, 26000] loss: 0.176\n",
            "[7,  2000] loss: 0.323\n",
            "[7,  4000] loss: 0.140\n",
            "[7,  6000] loss: 0.145\n",
            "[7,  8000] loss: 0.120\n",
            "[7, 10000] loss: 0.073\n",
            "[7, 12000] loss: 0.095\n",
            "[7, 14000] loss: 0.539\n",
            "[7, 16000] loss: 0.236\n",
            "[7, 18000] loss: 0.225\n",
            "[7, 20000] loss: 0.158\n",
            "[7, 22000] loss: 0.133\n",
            "[7, 24000] loss: 0.094\n",
            "[7, 26000] loss: 0.194\n",
            "[8,  2000] loss: 0.281\n",
            "[8,  4000] loss: 0.176\n",
            "[8,  6000] loss: 0.266\n",
            "[8,  8000] loss: 0.531\n",
            "[8, 10000] loss: 0.293\n",
            "[8, 12000] loss: 0.417\n",
            "[8, 14000] loss: 0.382\n",
            "[8, 16000] loss: 0.243\n",
            "[8, 18000] loss: 0.382\n",
            "[8, 20000] loss: 0.316\n",
            "[8, 22000] loss: 0.313\n",
            "[8, 24000] loss: 0.312\n",
            "[8, 26000] loss: 0.420\n",
            "[9,  2000] loss: 0.385\n",
            "[9,  4000] loss: 0.192\n",
            "[9,  6000] loss: 0.411\n",
            "[9,  8000] loss: 0.161\n",
            "[9, 10000] loss: 0.246\n",
            "[9, 12000] loss: 0.151\n",
            "[9, 14000] loss: 0.155\n",
            "[9, 16000] loss: 0.196\n",
            "[9, 18000] loss: 0.319\n",
            "[9, 20000] loss: 0.208\n",
            "[9, 22000] loss: 0.418\n",
            "[9, 24000] loss: 0.364\n",
            "[9, 26000] loss: 0.229\n",
            "[10,  2000] loss: 0.400\n",
            "[10,  4000] loss: 0.615\n",
            "[10,  6000] loss: 0.459\n",
            "[10,  8000] loss: 0.173\n",
            "[10, 10000] loss: 0.391\n",
            "[10, 12000] loss: 0.245\n",
            "[10, 14000] loss: 0.460\n",
            "[10, 16000] loss: 0.452\n",
            "[10, 18000] loss: 0.766\n",
            "[10, 20000] loss: 2.026\n",
            "[10, 22000] loss: 2.934\n",
            "[10, 24000] loss: 2.926\n",
            "[10, 26000] loss: 2.892\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.03148148148148148\n",
            "Training on fold 6/10...\n",
            "[1,  2000] loss: 2.890\n",
            "[1,  4000] loss: 2.893\n",
            "[1,  6000] loss: 2.890\n",
            "[1,  8000] loss: 2.893\n",
            "[1, 10000] loss: 2.892\n",
            "[1, 12000] loss: 2.893\n",
            "[1, 14000] loss: 2.894\n",
            "[1, 16000] loss: 2.893\n",
            "[1, 18000] loss: 2.893\n",
            "[1, 20000] loss: 2.892\n",
            "[1, 22000] loss: 2.890\n",
            "[1, 24000] loss: 2.894\n",
            "[1, 26000] loss: 2.892\n",
            "[2,  2000] loss: 2.890\n",
            "[2,  4000] loss: 2.893\n",
            "[2,  6000] loss: 2.891\n",
            "[2,  8000] loss: 2.893\n",
            "[2, 10000] loss: 2.892\n",
            "[2, 12000] loss: 2.893\n",
            "[2, 14000] loss: 2.894\n",
            "[2, 16000] loss: 2.893\n",
            "[2, 18000] loss: 2.893\n",
            "[2, 20000] loss: 2.892\n",
            "[2, 22000] loss: 2.890\n",
            "[2, 24000] loss: 2.894\n",
            "[2, 26000] loss: 2.892\n",
            "[3,  2000] loss: 2.890\n",
            "[3,  4000] loss: 2.893\n",
            "[3,  6000] loss: 2.891\n",
            "[3,  8000] loss: 2.893\n",
            "[3, 10000] loss: 2.892\n",
            "[3, 12000] loss: 2.893\n",
            "[3, 14000] loss: 2.894\n",
            "[3, 16000] loss: 2.893\n",
            "[3, 18000] loss: 2.893\n",
            "[3, 20000] loss: 2.892\n",
            "[3, 22000] loss: 2.890\n",
            "[3, 24000] loss: 2.894\n",
            "[3, 26000] loss: 2.892\n",
            "[4,  2000] loss: 2.890\n",
            "[4,  4000] loss: 2.893\n",
            "[4,  6000] loss: 2.891\n",
            "[4,  8000] loss: 2.893\n",
            "[4, 10000] loss: 2.892\n",
            "[4, 12000] loss: 2.893\n",
            "[4, 14000] loss: 2.894\n",
            "[4, 16000] loss: 2.893\n",
            "[4, 18000] loss: 2.893\n",
            "[4, 20000] loss: 2.892\n",
            "[4, 22000] loss: 2.890\n",
            "[4, 24000] loss: 2.894\n",
            "[4, 26000] loss: 2.892\n",
            "[5,  2000] loss: 2.890\n",
            "[5,  4000] loss: 2.893\n",
            "[5,  6000] loss: 2.891\n",
            "[5,  8000] loss: 2.893\n",
            "[5, 10000] loss: 2.892\n",
            "[5, 12000] loss: 2.893\n",
            "[5, 14000] loss: 2.894\n",
            "[5, 16000] loss: 2.893\n",
            "[5, 18000] loss: 2.893\n",
            "[5, 20000] loss: 2.892\n",
            "[5, 22000] loss: 2.890\n",
            "[5, 24000] loss: 2.894\n",
            "[5, 26000] loss: 2.892\n",
            "[6,  2000] loss: 2.890\n",
            "[6,  4000] loss: 2.893\n",
            "[6,  6000] loss: 2.891\n",
            "[6,  8000] loss: 2.893\n",
            "[6, 10000] loss: 2.892\n",
            "[6, 12000] loss: 2.893\n",
            "[6, 14000] loss: 2.894\n",
            "[6, 16000] loss: 2.893\n",
            "[6, 18000] loss: 2.893\n",
            "[6, 20000] loss: 2.892\n",
            "[6, 22000] loss: 2.890\n",
            "[6, 24000] loss: 2.894\n",
            "[6, 26000] loss: 2.892\n",
            "[7,  2000] loss: 2.890\n",
            "[7,  4000] loss: 2.893\n",
            "[7,  6000] loss: 2.891\n",
            "[7,  8000] loss: 2.893\n",
            "[7, 10000] loss: 2.892\n",
            "[7, 12000] loss: 2.893\n",
            "[7, 14000] loss: 2.894\n",
            "[7, 16000] loss: 2.893\n",
            "[7, 18000] loss: 2.893\n",
            "[7, 20000] loss: 2.892\n",
            "[7, 22000] loss: 2.890\n",
            "[7, 24000] loss: 2.894\n",
            "[7, 26000] loss: 2.892\n",
            "[8,  2000] loss: 2.890\n",
            "[8,  4000] loss: 2.893\n",
            "[8,  6000] loss: 2.891\n",
            "[8,  8000] loss: 2.893\n",
            "[8, 10000] loss: 2.892\n",
            "[8, 12000] loss: 2.893\n",
            "[8, 14000] loss: 2.894\n",
            "[8, 16000] loss: 2.893\n",
            "[8, 18000] loss: 2.893\n",
            "[8, 20000] loss: 2.892\n",
            "[8, 22000] loss: 2.890\n",
            "[8, 24000] loss: 2.894\n",
            "[8, 26000] loss: 2.892\n",
            "[9,  2000] loss: 2.890\n",
            "[9,  4000] loss: 2.893\n",
            "[9,  6000] loss: 2.891\n",
            "[9,  8000] loss: 2.893\n",
            "[9, 10000] loss: 2.892\n",
            "[9, 12000] loss: 2.893\n",
            "[9, 14000] loss: 2.894\n",
            "[9, 16000] loss: 2.893\n",
            "[9, 18000] loss: 2.893\n",
            "[9, 20000] loss: 2.892\n",
            "[9, 22000] loss: 2.890\n",
            "[9, 24000] loss: 2.894\n",
            "[9, 26000] loss: 2.892\n",
            "[10,  2000] loss: 2.890\n",
            "[10,  4000] loss: 2.893\n",
            "[10,  6000] loss: 2.891\n",
            "[10,  8000] loss: 2.893\n",
            "[10, 10000] loss: 2.892\n",
            "[10, 12000] loss: 2.893\n",
            "[10, 14000] loss: 2.894\n",
            "[10, 16000] loss: 2.893\n",
            "[10, 18000] loss: 2.893\n",
            "[10, 20000] loss: 2.892\n",
            "[10, 22000] loss: 2.890\n",
            "[10, 24000] loss: 2.894\n",
            "[10, 26000] loss: 2.892\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.03148148148148148\n",
            "Training on fold 7/10...\n",
            "[1,  2000] loss: 2.890\n",
            "[1,  4000] loss: 2.892\n",
            "[1,  6000] loss: 2.891\n",
            "[1,  8000] loss: 2.893\n",
            "[1, 10000] loss: 2.893\n",
            "[1, 12000] loss: 2.892\n",
            "[1, 14000] loss: 2.894\n",
            "[1, 16000] loss: 2.893\n",
            "[1, 18000] loss: 2.893\n",
            "[1, 20000] loss: 2.894\n",
            "[1, 22000] loss: 2.891\n",
            "[1, 24000] loss: 2.893\n",
            "[1, 26000] loss: 2.892\n",
            "[2,  2000] loss: 2.891\n",
            "[2,  4000] loss: 2.892\n",
            "[2,  6000] loss: 2.892\n",
            "[2,  8000] loss: 2.893\n",
            "[2, 10000] loss: 2.893\n",
            "[2, 12000] loss: 2.892\n",
            "[2, 14000] loss: 2.894\n",
            "[2, 16000] loss: 2.893\n",
            "[2, 18000] loss: 2.893\n",
            "[2, 20000] loss: 2.894\n",
            "[2, 22000] loss: 2.891\n",
            "[2, 24000] loss: 2.893\n",
            "[2, 26000] loss: 2.892\n",
            "[3,  2000] loss: 2.891\n",
            "[3,  4000] loss: 2.892\n",
            "[3,  6000] loss: 2.892\n",
            "[3,  8000] loss: 2.893\n",
            "[3, 10000] loss: 2.893\n",
            "[3, 12000] loss: 2.892\n",
            "[3, 14000] loss: 2.894\n",
            "[3, 16000] loss: 2.893\n",
            "[3, 18000] loss: 2.893\n",
            "[3, 20000] loss: 2.894\n",
            "[3, 22000] loss: 2.891\n",
            "[3, 24000] loss: 2.893\n",
            "[3, 26000] loss: 2.892\n",
            "[4,  2000] loss: 2.891\n",
            "[4,  4000] loss: 2.892\n",
            "[4,  6000] loss: 2.892\n",
            "[4,  8000] loss: 2.893\n",
            "[4, 10000] loss: 2.893\n",
            "[4, 12000] loss: 2.892\n",
            "[4, 14000] loss: 2.894\n",
            "[4, 16000] loss: 2.893\n",
            "[4, 18000] loss: 2.893\n",
            "[4, 20000] loss: 2.894\n",
            "[4, 22000] loss: 2.891\n",
            "[4, 24000] loss: 2.893\n",
            "[4, 26000] loss: 2.892\n",
            "[5,  2000] loss: 2.891\n",
            "[5,  4000] loss: 2.892\n",
            "[5,  6000] loss: 2.892\n",
            "[5,  8000] loss: 2.893\n",
            "[5, 10000] loss: 2.893\n",
            "[5, 12000] loss: 2.892\n",
            "[5, 14000] loss: 2.894\n",
            "[5, 16000] loss: 2.893\n",
            "[5, 18000] loss: 2.893\n",
            "[5, 20000] loss: 2.894\n",
            "[5, 22000] loss: 2.891\n",
            "[5, 24000] loss: 2.893\n",
            "[5, 26000] loss: 2.892\n",
            "[6,  2000] loss: 2.891\n",
            "[6,  4000] loss: 2.892\n",
            "[6,  6000] loss: 2.892\n",
            "[6,  8000] loss: 2.893\n",
            "[6, 10000] loss: 2.893\n",
            "[6, 12000] loss: 2.892\n",
            "[6, 14000] loss: 2.894\n",
            "[6, 16000] loss: 2.893\n",
            "[6, 18000] loss: 2.893\n",
            "[6, 20000] loss: 2.894\n",
            "[6, 22000] loss: 2.891\n",
            "[6, 24000] loss: 2.893\n",
            "[6, 26000] loss: 2.892\n",
            "[7,  2000] loss: 2.891\n",
            "[7,  4000] loss: 2.892\n",
            "[7,  6000] loss: 2.892\n",
            "[7,  8000] loss: 2.893\n",
            "[7, 10000] loss: 2.893\n",
            "[7, 12000] loss: 2.892\n",
            "[7, 14000] loss: 2.894\n",
            "[7, 16000] loss: 2.893\n",
            "[7, 18000] loss: 2.893\n",
            "[7, 20000] loss: 2.894\n",
            "[7, 22000] loss: 2.891\n",
            "[7, 24000] loss: 2.893\n",
            "[7, 26000] loss: 2.892\n",
            "[8,  2000] loss: 2.891\n",
            "[8,  4000] loss: 2.892\n",
            "[8,  6000] loss: 2.892\n",
            "[8,  8000] loss: 2.893\n",
            "[8, 10000] loss: 2.893\n",
            "[8, 12000] loss: 2.892\n",
            "[8, 14000] loss: 2.894\n",
            "[8, 16000] loss: 2.893\n",
            "[8, 18000] loss: 2.893\n",
            "[8, 20000] loss: 2.894\n",
            "[8, 22000] loss: 2.891\n",
            "[8, 24000] loss: 2.893\n",
            "[8, 26000] loss: 2.892\n",
            "[9,  2000] loss: 2.891\n",
            "[9,  4000] loss: 2.892\n",
            "[9,  6000] loss: 2.892\n",
            "[9,  8000] loss: 2.893\n",
            "[9, 10000] loss: 2.893\n",
            "[9, 12000] loss: 2.892\n",
            "[9, 14000] loss: 2.894\n",
            "[9, 16000] loss: 2.893\n",
            "[9, 18000] loss: 2.893\n",
            "[9, 20000] loss: 2.894\n",
            "[9, 22000] loss: 2.891\n",
            "[9, 24000] loss: 2.893\n",
            "[9, 26000] loss: 2.892\n",
            "[10,  2000] loss: 2.891\n",
            "[10,  4000] loss: 2.892\n",
            "[10,  6000] loss: 2.892\n",
            "[10,  8000] loss: 2.893\n",
            "[10, 10000] loss: 2.893\n",
            "[10, 12000] loss: 2.892\n",
            "[10, 14000] loss: 2.894\n",
            "[10, 16000] loss: 2.893\n",
            "[10, 18000] loss: 2.893\n",
            "[10, 20000] loss: 2.894\n",
            "[10, 22000] loss: 2.891\n",
            "[10, 24000] loss: 2.893\n",
            "[10, 26000] loss: 2.892\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.03148148148148148\n",
            "Training on fold 8/10...\n",
            "[1,  2000] loss: 2.891\n",
            "[1,  4000] loss: 2.893\n",
            "[1,  6000] loss: 2.892\n",
            "[1,  8000] loss: 2.893\n",
            "[1, 10000] loss: 2.893\n",
            "[1, 12000] loss: 2.893\n",
            "[1, 14000] loss: 2.893\n",
            "[1, 16000] loss: 2.892\n",
            "[1, 18000] loss: 2.894\n",
            "[1, 20000] loss: 2.893\n",
            "[1, 22000] loss: 2.891\n",
            "[1, 24000] loss: 2.893\n",
            "[1, 26000] loss: 2.892\n",
            "[2,  2000] loss: 2.890\n",
            "[2,  4000] loss: 2.893\n",
            "[2,  6000] loss: 2.892\n",
            "[2,  8000] loss: 2.893\n",
            "[2, 10000] loss: 2.893\n",
            "[2, 12000] loss: 2.893\n",
            "[2, 14000] loss: 2.893\n",
            "[2, 16000] loss: 2.892\n",
            "[2, 18000] loss: 2.894\n",
            "[2, 20000] loss: 2.893\n",
            "[2, 22000] loss: 2.891\n",
            "[2, 24000] loss: 2.893\n",
            "[2, 26000] loss: 2.892\n",
            "[3,  2000] loss: 2.890\n",
            "[3,  4000] loss: 2.893\n",
            "[3,  6000] loss: 2.892\n",
            "[3,  8000] loss: 2.893\n",
            "[3, 10000] loss: 2.893\n",
            "[3, 12000] loss: 2.893\n",
            "[3, 14000] loss: 2.893\n",
            "[3, 16000] loss: 2.892\n",
            "[3, 18000] loss: 2.894\n",
            "[3, 20000] loss: 2.893\n",
            "[3, 22000] loss: 2.891\n",
            "[3, 24000] loss: 2.893\n",
            "[3, 26000] loss: 2.892\n",
            "[4,  2000] loss: 2.890\n",
            "[4,  4000] loss: 2.893\n",
            "[4,  6000] loss: 2.892\n",
            "[4,  8000] loss: 2.893\n",
            "[4, 10000] loss: 2.893\n",
            "[4, 12000] loss: 2.893\n",
            "[4, 14000] loss: 2.893\n",
            "[4, 16000] loss: 2.892\n",
            "[4, 18000] loss: 2.894\n",
            "[4, 20000] loss: 2.893\n",
            "[4, 22000] loss: 2.891\n",
            "[4, 24000] loss: 2.893\n",
            "[4, 26000] loss: 2.892\n",
            "[5,  2000] loss: 2.890\n",
            "[5,  4000] loss: 2.893\n",
            "[5,  6000] loss: 2.892\n",
            "[5,  8000] loss: 2.893\n",
            "[5, 10000] loss: 2.893\n",
            "[5, 12000] loss: 2.893\n",
            "[5, 14000] loss: 2.893\n",
            "[5, 16000] loss: 2.892\n",
            "[5, 18000] loss: 2.894\n",
            "[5, 20000] loss: 2.893\n",
            "[5, 22000] loss: 2.891\n",
            "[5, 24000] loss: 2.893\n",
            "[5, 26000] loss: 2.892\n",
            "[6,  2000] loss: 2.890\n",
            "[6,  4000] loss: 2.893\n",
            "[6,  6000] loss: 2.892\n",
            "[6,  8000] loss: 2.893\n",
            "[6, 10000] loss: 2.893\n",
            "[6, 12000] loss: 2.893\n",
            "[6, 14000] loss: 2.893\n",
            "[6, 16000] loss: 2.892\n",
            "[6, 18000] loss: 2.894\n",
            "[6, 20000] loss: 2.893\n",
            "[6, 22000] loss: 2.891\n",
            "[6, 24000] loss: 2.893\n",
            "[6, 26000] loss: 2.892\n",
            "[7,  2000] loss: 2.890\n",
            "[7,  4000] loss: 2.893\n",
            "[7,  6000] loss: 2.892\n",
            "[7,  8000] loss: 2.893\n",
            "[7, 10000] loss: 2.893\n",
            "[7, 12000] loss: 2.893\n",
            "[7, 14000] loss: 2.893\n",
            "[7, 16000] loss: 2.892\n",
            "[7, 18000] loss: 2.894\n",
            "[7, 20000] loss: 2.893\n",
            "[7, 22000] loss: 2.891\n",
            "[7, 24000] loss: 2.893\n",
            "[7, 26000] loss: 2.892\n",
            "[8,  2000] loss: 2.890\n",
            "[8,  4000] loss: 2.893\n",
            "[8,  6000] loss: 2.892\n",
            "[8,  8000] loss: 2.893\n",
            "[8, 10000] loss: 2.893\n",
            "[8, 12000] loss: 2.893\n",
            "[8, 14000] loss: 2.893\n",
            "[8, 16000] loss: 2.892\n",
            "[8, 18000] loss: 2.894\n",
            "[8, 20000] loss: 2.893\n",
            "[8, 22000] loss: 2.891\n",
            "[8, 24000] loss: 2.893\n",
            "[8, 26000] loss: 2.892\n",
            "[9,  2000] loss: 2.890\n",
            "[9,  4000] loss: 2.893\n",
            "[9,  6000] loss: 2.892\n",
            "[9,  8000] loss: 2.893\n",
            "[9, 10000] loss: 2.893\n",
            "[9, 12000] loss: 2.893\n",
            "[9, 14000] loss: 2.893\n",
            "[9, 16000] loss: 2.892\n",
            "[9, 18000] loss: 2.894\n",
            "[9, 20000] loss: 2.893\n",
            "[9, 22000] loss: 2.891\n",
            "[9, 24000] loss: 2.893\n",
            "[9, 26000] loss: 2.892\n",
            "[10,  2000] loss: 2.890\n",
            "[10,  4000] loss: 2.893\n",
            "[10,  6000] loss: 2.892\n",
            "[10,  8000] loss: 2.893\n",
            "[10, 10000] loss: 2.893\n",
            "[10, 12000] loss: 2.893\n",
            "[10, 14000] loss: 2.893\n",
            "[10, 16000] loss: 2.892\n",
            "[10, 18000] loss: 2.894\n",
            "[10, 20000] loss: 2.893\n",
            "[10, 22000] loss: 2.891\n",
            "[10, 24000] loss: 2.893\n",
            "[10, 26000] loss: 2.892\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.03148148148148148\n",
            "Training on fold 9/10...\n",
            "[1,  2000] loss: 2.891\n",
            "[1,  4000] loss: 2.893\n",
            "[1,  6000] loss: 2.892\n",
            "[1,  8000] loss: 2.894\n",
            "[1, 10000] loss: 2.894\n",
            "[1, 12000] loss: 2.892\n",
            "[1, 14000] loss: 2.894\n",
            "[1, 16000] loss: 2.892\n",
            "[1, 18000] loss: 2.893\n",
            "[1, 20000] loss: 2.893\n",
            "[1, 22000] loss: 2.891\n",
            "[1, 24000] loss: 2.894\n",
            "[1, 26000] loss: 2.892\n",
            "[2,  2000] loss: 2.891\n",
            "[2,  4000] loss: 2.893\n",
            "[2,  6000] loss: 2.892\n",
            "[2,  8000] loss: 2.894\n",
            "[2, 10000] loss: 2.894\n",
            "[2, 12000] loss: 2.892\n",
            "[2, 14000] loss: 2.894\n",
            "[2, 16000] loss: 2.892\n",
            "[2, 18000] loss: 2.893\n",
            "[2, 20000] loss: 2.893\n",
            "[2, 22000] loss: 2.891\n",
            "[2, 24000] loss: 2.894\n",
            "[2, 26000] loss: 2.892\n",
            "[3,  2000] loss: 2.891\n",
            "[3,  4000] loss: 2.893\n",
            "[3,  6000] loss: 2.892\n",
            "[3,  8000] loss: 2.894\n",
            "[3, 10000] loss: 2.894\n",
            "[3, 12000] loss: 2.892\n",
            "[3, 14000] loss: 2.894\n",
            "[3, 16000] loss: 2.892\n",
            "[3, 18000] loss: 2.893\n",
            "[3, 20000] loss: 2.893\n",
            "[3, 22000] loss: 2.891\n",
            "[3, 24000] loss: 2.894\n",
            "[3, 26000] loss: 2.892\n",
            "[4,  2000] loss: 2.891\n",
            "[4,  4000] loss: 2.893\n",
            "[4,  6000] loss: 2.892\n",
            "[4,  8000] loss: 2.894\n",
            "[4, 10000] loss: 2.894\n",
            "[4, 12000] loss: 2.892\n",
            "[4, 14000] loss: 2.894\n",
            "[4, 16000] loss: 2.892\n",
            "[4, 18000] loss: 2.893\n",
            "[4, 20000] loss: 2.893\n",
            "[4, 22000] loss: 2.891\n",
            "[4, 24000] loss: 2.894\n",
            "[4, 26000] loss: 2.892\n",
            "[5,  2000] loss: 2.891\n",
            "[5,  4000] loss: 2.893\n",
            "[5,  6000] loss: 2.892\n",
            "[5,  8000] loss: 2.894\n",
            "[5, 10000] loss: 2.894\n",
            "[5, 12000] loss: 2.892\n",
            "[5, 14000] loss: 2.894\n",
            "[5, 16000] loss: 2.892\n",
            "[5, 18000] loss: 2.893\n",
            "[5, 20000] loss: 2.893\n",
            "[5, 22000] loss: 2.891\n",
            "[5, 24000] loss: 2.894\n",
            "[5, 26000] loss: 2.892\n",
            "[6,  2000] loss: 2.891\n",
            "[6,  4000] loss: 2.893\n",
            "[6,  6000] loss: 2.892\n",
            "[6,  8000] loss: 2.894\n",
            "[6, 10000] loss: 2.894\n",
            "[6, 12000] loss: 2.892\n",
            "[6, 14000] loss: 2.894\n",
            "[6, 16000] loss: 2.892\n",
            "[6, 18000] loss: 2.893\n",
            "[6, 20000] loss: 2.893\n",
            "[6, 22000] loss: 2.891\n",
            "[6, 24000] loss: 2.894\n",
            "[6, 26000] loss: 2.892\n",
            "[7,  2000] loss: 2.891\n",
            "[7,  4000] loss: 2.893\n",
            "[7,  6000] loss: 2.892\n",
            "[7,  8000] loss: 2.894\n",
            "[7, 10000] loss: 2.894\n",
            "[7, 12000] loss: 2.892\n",
            "[7, 14000] loss: 2.894\n",
            "[7, 16000] loss: 2.892\n",
            "[7, 18000] loss: 2.893\n",
            "[7, 20000] loss: 2.893\n",
            "[7, 22000] loss: 2.891\n",
            "[7, 24000] loss: 2.894\n",
            "[7, 26000] loss: 2.892\n",
            "[8,  2000] loss: 2.891\n",
            "[8,  4000] loss: 2.893\n",
            "[8,  6000] loss: 2.892\n",
            "[8,  8000] loss: 2.894\n",
            "[8, 10000] loss: 2.894\n",
            "[8, 12000] loss: 2.892\n",
            "[8, 14000] loss: 2.894\n",
            "[8, 16000] loss: 2.892\n",
            "[8, 18000] loss: 2.893\n",
            "[8, 20000] loss: 2.893\n",
            "[8, 22000] loss: 2.891\n",
            "[8, 24000] loss: 2.894\n",
            "[8, 26000] loss: 2.892\n",
            "[9,  2000] loss: 2.891\n",
            "[9,  4000] loss: 2.893\n",
            "[9,  6000] loss: 2.892\n",
            "[9,  8000] loss: 2.894\n",
            "[9, 10000] loss: 2.894\n",
            "[9, 12000] loss: 2.892\n",
            "[9, 14000] loss: 2.894\n",
            "[9, 16000] loss: 2.892\n",
            "[9, 18000] loss: 2.893\n",
            "[9, 20000] loss: 2.893\n",
            "[9, 22000] loss: 2.891\n",
            "[9, 24000] loss: 2.894\n",
            "[9, 26000] loss: 2.892\n",
            "[10,  2000] loss: 2.891\n",
            "[10,  4000] loss: 2.893\n",
            "[10,  6000] loss: 2.892\n",
            "[10,  8000] loss: 2.894\n",
            "[10, 10000] loss: 2.894\n",
            "[10, 12000] loss: 2.892\n",
            "[10, 14000] loss: 2.894\n",
            "[10, 16000] loss: 2.892\n",
            "[10, 18000] loss: 2.893\n",
            "[10, 20000] loss: 2.893\n",
            "[10, 22000] loss: 2.891\n",
            "[10, 24000] loss: 2.894\n",
            "[10, 26000] loss: 2.892\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.03148148148148148\n",
            "Training on fold 10/10...\n",
            "[1,  2000] loss: 2.890\n",
            "[1,  4000] loss: 2.894\n",
            "[1,  6000] loss: 2.892\n",
            "[1,  8000] loss: 2.894\n",
            "[1, 10000] loss: 2.893\n",
            "[1, 12000] loss: 2.892\n",
            "[1, 14000] loss: 2.894\n",
            "[1, 16000] loss: 2.893\n",
            "[1, 18000] loss: 2.893\n",
            "[1, 20000] loss: 2.893\n",
            "[1, 22000] loss: 2.891\n",
            "[1, 24000] loss: 2.893\n",
            "[1, 26000] loss: 2.892\n",
            "[2,  2000] loss: 2.890\n",
            "[2,  4000] loss: 2.894\n",
            "[2,  6000] loss: 2.892\n",
            "[2,  8000] loss: 2.894\n",
            "[2, 10000] loss: 2.893\n",
            "[2, 12000] loss: 2.892\n",
            "[2, 14000] loss: 2.894\n",
            "[2, 16000] loss: 2.893\n",
            "[2, 18000] loss: 2.893\n",
            "[2, 20000] loss: 2.893\n",
            "[2, 22000] loss: 2.891\n",
            "[2, 24000] loss: 2.893\n",
            "[2, 26000] loss: 2.892\n",
            "[3,  2000] loss: 2.890\n",
            "[3,  4000] loss: 2.894\n",
            "[3,  6000] loss: 2.892\n",
            "[3,  8000] loss: 2.894\n",
            "[3, 10000] loss: 2.893\n",
            "[3, 12000] loss: 2.892\n",
            "[3, 14000] loss: 2.894\n",
            "[3, 16000] loss: 2.893\n",
            "[3, 18000] loss: 2.893\n",
            "[3, 20000] loss: 2.893\n",
            "[3, 22000] loss: 2.891\n",
            "[3, 24000] loss: 2.893\n",
            "[3, 26000] loss: 2.892\n",
            "[4,  2000] loss: 2.890\n",
            "[4,  4000] loss: 2.894\n",
            "[4,  6000] loss: 2.892\n",
            "[4,  8000] loss: 2.894\n",
            "[4, 10000] loss: 2.893\n",
            "[4, 12000] loss: 2.892\n",
            "[4, 14000] loss: 2.894\n",
            "[4, 16000] loss: 2.893\n",
            "[4, 18000] loss: 2.893\n",
            "[4, 20000] loss: 2.893\n",
            "[4, 22000] loss: 2.891\n",
            "[4, 24000] loss: 2.893\n",
            "[4, 26000] loss: 2.892\n",
            "[5,  2000] loss: 2.890\n",
            "[5,  4000] loss: 2.894\n",
            "[5,  6000] loss: 2.892\n",
            "[5,  8000] loss: 2.894\n",
            "[5, 10000] loss: 2.893\n",
            "[5, 12000] loss: 2.892\n",
            "[5, 14000] loss: 2.894\n",
            "[5, 16000] loss: 2.893\n",
            "[5, 18000] loss: 2.893\n",
            "[5, 20000] loss: 2.893\n",
            "[5, 22000] loss: 2.891\n",
            "[5, 24000] loss: 2.893\n",
            "[5, 26000] loss: 2.892\n",
            "[6,  2000] loss: 2.890\n",
            "[6,  4000] loss: 2.894\n",
            "[6,  6000] loss: 2.892\n",
            "[6,  8000] loss: 2.894\n",
            "[6, 10000] loss: 2.893\n",
            "[6, 12000] loss: 2.892\n",
            "[6, 14000] loss: 2.894\n",
            "[6, 16000] loss: 2.893\n",
            "[6, 18000] loss: 2.893\n",
            "[6, 20000] loss: 2.893\n",
            "[6, 22000] loss: 2.891\n",
            "[6, 24000] loss: 2.893\n",
            "[6, 26000] loss: 2.892\n",
            "[7,  2000] loss: 2.890\n",
            "[7,  4000] loss: 2.894\n",
            "[7,  6000] loss: 2.892\n",
            "[7,  8000] loss: 2.894\n",
            "[7, 10000] loss: 2.893\n",
            "[7, 12000] loss: 2.892\n",
            "[7, 14000] loss: 2.894\n",
            "[7, 16000] loss: 2.893\n",
            "[7, 18000] loss: 2.893\n",
            "[7, 20000] loss: 2.893\n",
            "[7, 22000] loss: 2.891\n",
            "[7, 24000] loss: 2.893\n",
            "[7, 26000] loss: 2.892\n",
            "[8,  2000] loss: 2.890\n",
            "[8,  4000] loss: 2.894\n",
            "[8,  6000] loss: 2.892\n",
            "[8,  8000] loss: 2.894\n",
            "[8, 10000] loss: 2.893\n",
            "[8, 12000] loss: 2.892\n",
            "[8, 14000] loss: 2.894\n",
            "[8, 16000] loss: 2.893\n",
            "[8, 18000] loss: 2.893\n",
            "[8, 20000] loss: 2.893\n",
            "[8, 22000] loss: 2.891\n",
            "[8, 24000] loss: 2.893\n",
            "[8, 26000] loss: 2.892\n",
            "[9,  2000] loss: 2.890\n",
            "[9,  4000] loss: 2.894\n",
            "[9,  6000] loss: 2.892\n",
            "[9,  8000] loss: 2.894\n",
            "[9, 10000] loss: 2.893\n",
            "[9, 12000] loss: 2.892\n",
            "[9, 14000] loss: 2.894\n",
            "[9, 16000] loss: 2.893\n",
            "[9, 18000] loss: 2.893\n",
            "[9, 20000] loss: 2.893\n",
            "[9, 22000] loss: 2.891\n",
            "[9, 24000] loss: 2.893\n",
            "[9, 26000] loss: 2.892\n",
            "[10,  2000] loss: 2.890\n",
            "[10,  4000] loss: 2.894\n",
            "[10,  6000] loss: 2.892\n",
            "[10,  8000] loss: 2.894\n",
            "[10, 10000] loss: 2.893\n",
            "[10, 12000] loss: 2.892\n",
            "[10, 14000] loss: 2.894\n",
            "[10, 16000] loss: 2.893\n",
            "[10, 18000] loss: 2.893\n",
            "[10, 20000] loss: 2.893\n",
            "[10, 22000] loss: 2.891\n",
            "[10, 24000] loss: 2.893\n",
            "[10, 26000] loss: 2.892\n",
            "Finished Training\n",
            "Training new iteration on 27540 training samples, 3060 validation samples\n",
            "(5400, 1, 32, 32)\n",
            "Accuracy=0.03148148148148148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2chr2k6VII-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}